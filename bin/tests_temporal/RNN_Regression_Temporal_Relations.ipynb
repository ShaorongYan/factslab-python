{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105822a70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import LSTM\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss, CrossEntropyLoss\n",
    "from scipy.special import huber\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Iterable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '/Users/sidvash/facts_lab/factslab-python/factslab/')\n",
    "from utility import load_glove_embedding\n",
    "from datastructures import ConstituencyTree\n",
    "from pytorch.childsumtreelstm import *\n",
    "#from pytorch.rnnregression import RNNRegressionTrainer\n",
    "from pytorch.rnnregression import RNNRegression\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from MTurK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data locations:\n",
    "data_file = \"/Users/sidvash/facts_lab/factslab-protocols-eventtemporal/\" \n",
    "data_file += \"Temporal_relations/testing_HIT/agreement_data_protocol2.csv\"\n",
    "\n",
    "turk_output = \"/Users/sidvash/facts_lab/factslab-protocols-eventtemporal/\" \n",
    "turk_output += \"Temporal_relations/testing_HIT/Batch_3220834_batch_results.csv\"\n",
    "\n",
    "embed_path = '/Users/sidvash/kaggle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence(s):\n",
    "    '''\n",
    "    Filter out html <span> tags from a sentence \n",
    "    '''\n",
    "    \n",
    "    s = re.sub(r'\\<span class\\=\\\\\"predicate\\\\\">', r'', s)\n",
    "    \n",
    "    s = re.sub(r'\\<\\/span\\>', r' ', s)\n",
    "    \n",
    "    s = re.sub(r'  ', r'', s) #remove extra spaces\n",
    "\n",
    "    return s\n",
    "\n",
    "def extract_dataframe(data):\n",
    "    '''\n",
    "    Input: Pandas csv dataframe obtained from MTurk\n",
    "    \n",
    "    Output: Pandas dataframe levelled by (User x Sentenced_ID)\n",
    "    '''\n",
    "    data[\"dicts\"] = data[\"Input_var_arrays\"].map(lambda x: json.loads(x))\n",
    "    global_list = []\n",
    "    \n",
    "    for row in data.itertuples():\n",
    "        for idx, local_dict in enumerate(row.dicts):\n",
    "            temp_dict = local_dict.copy()\n",
    "            var_pred1 = \"Answer_sld_pred1_\" + str(idx+1)\n",
    "            var_pred2 =  \"Answer_sld_pred2_\" + str(idx+1)\n",
    "            var_conf = \"Answer_confidence_range\" + str(idx+1)\n",
    "            \n",
    "            temp_dict[\"slider1_posn\"] = getattr(row, var_pred1)\n",
    "            temp_dict[\"slider2_posn\"] = getattr(row, var_pred2)\n",
    "            temp_dict[\"confidence\"] = getattr(row, var_conf)\n",
    "            temp_dict[\"worker_id\"] = row.WorkerId\n",
    "            temp_dict[\"hit_id\"] = row.HITId\n",
    "\n",
    "            global_list.append(temp_dict)\n",
    "            \n",
    "    return pd.DataFrame(global_list)\n",
    "\n",
    "def time_ml_tag(row, var1, var2):\n",
    "    '''\n",
    "    Creates time ML tags from slider positions of two events \n",
    "    \n",
    "    Eg: EVENT 1 \"is before\" EVENT2\n",
    "    \n",
    "    Eg: Relatins: is before\n",
    "    '''\n",
    "    ans = \"\"\n",
    "    \n",
    "    slider1 = getattr(row, var1)\n",
    "    slider2 = getattr(row, var2)\n",
    "    \n",
    "    event1 = [int(x) for x in slider1.split(\"-\")]\n",
    "    event2 = [int(x) for x in slider2.split(\"-\")]\n",
    "    \n",
    "    if (event1[0] == event2[0]) and (event1[1] == event2[1]):\n",
    "        ans = \"simultaneous\"\n",
    "        \n",
    "    elif event1[1] <= event2[0]:\n",
    "        ans = \"before\"\n",
    "    \n",
    "    elif event1[0] >= event2[1]:\n",
    "        ans = \"after\"\n",
    "        \n",
    "    elif event1[1] >= event2[1] and event1[0] <= event2[0]:\n",
    "        ans = \"includes\"\n",
    "        \n",
    "    elif event1[1] <= event2[1] and event1[0] >= event2[0]:\n",
    "        ans = \"is_included\"\n",
    "    \n",
    "    elif event1[0] < event2[0] and event1[1] > event2[0]:\n",
    "        ans = \"before_cont\"\n",
    "        \n",
    "    elif event1[0] < event2[1] and event1[1] > event2[1]:\n",
    "        ans = \"after_cont\"\n",
    "    \n",
    "    else: \n",
    "        ans = \"other\"\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(turk_output)\n",
    "data.columns = [c.replace('.', '_') for c in data.columns]\n",
    "\n",
    "pilot_data = extract_dataframe(data)\n",
    "\n",
    "#Re-arrange the order \n",
    "cols = list(pilot_data.columns)\n",
    "#pilot_data = pilot_data[pilot_data.columns[::-1]].drop('sentence', axis=1)\n",
    "pilot_data = pilot_data[pilot_data.columns[::-1]]\n",
    "\n",
    "\n",
    "#Drop start, end, instant variables that were stored in the json data\n",
    "pilot_data = pilot_data.drop(['start_pred1', 'start_pred2', 'end_pred1', 'end_pred2',\n",
    "                             'instant_pred1', 'instant_pred2'], axis=1)\n",
    "\n",
    "\n",
    "#pilot_data.replace('na',np.NaN, inplace=True)\n",
    "pilot_data.head\n",
    "pilot_data['sent_token'] = pilot_data['sentence_id'] + \"_\" +\\\n",
    "                            pilot_data['pred_token1'].map(lambda x: str(x)) + \"_\" +\\\n",
    "                             pilot_data['pred_token2'].map(lambda x: str(x))\n",
    "pilot_data['timeML_tag'] = pilot_data.apply(lambda row: time_ml_tag(row, 'slider1_posn', 'slider2_posn'), axis=1)\n",
    "\n",
    "#pilot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data features\n",
    "pilot_data['filter_sent'] = pilot_data['sentence'].map(lambda x: [filter_sentence(x).split(\" \")])\n",
    "pilot_data['pred1_token_int'] = pilot_data['sent_token'].map(lambda x: int(x.split(\"_\")[-2]) + 1)\n",
    "pilot_data['pred2_token_int'] = pilot_data['sent_token'].map(lambda x: int(x.split(\"_\")[-1]) + 1)\n",
    "pilot_data['pred_tokens'] = pilot_data.apply(lambda row: [[row.pred1_token_int, row.pred2_token_int]], axis=1)\n",
    "\n",
    "pilot_data['slider1_start'] = pilot_data['slider1_posn'].map(lambda x: [int(ch) for ch in x.split(\"-\")][0])\n",
    "pilot_data['slider1_end'] = pilot_data['slider1_posn'].map(lambda x: [int(ch) for ch in x.split(\"-\")][1])\n",
    "\n",
    "pilot_data['slider2_start'] = pilot_data['slider2_posn'].map(lambda x: [int(ch) for ch in x.split(\"-\")][0])\n",
    "pilot_data['slider2_end'] = pilot_data['slider2_posn'].map(lambda x: [int(ch) for ch in x.split(\"-\")][1])\n",
    "#Convert objects to numerics\n",
    "\n",
    "X_data = pilot_data[['filter_sent','pred1_token_int','pred2_token_int', 'pred_tokens']]\n",
    "y_data = pilot_data[['slider1_start', 'slider1_end', 'slider2_start', 'slider2_end']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_sent</th>\n",
       "      <th>pred1_token_int</th>\n",
       "      <th>pred2_token_int</th>\n",
       "      <th>pred_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[On, Wednesday, guerrillas, had, kidnapped, a...</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>[[5, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[Yesterday, there, were, tens, of, them, putt...</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>[[7, 27]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[He, did, once, make, an, unforgivable, error...</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>[[4, 15]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[He, added, that, &amp;quot;, America, does, not,...</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>[[2, 58]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[As, a, child, in, the, 50&amp;#39;s, I, had, a, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>[[8, 35]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         filter_sent  pred1_token_int  \\\n",
       "0  [[On, Wednesday, guerrillas, had, kidnapped, a...                5   \n",
       "1  [[Yesterday, there, were, tens, of, them, putt...                7   \n",
       "2  [[He, did, once, make, an, unforgivable, error...                4   \n",
       "3  [[He, added, that, &quot;, America, does, not,...                2   \n",
       "4  [[As, a, child, in, the, 50&#39;s, I, had, a, ...                8   \n",
       "\n",
       "   pred2_token_int pred_tokens  \n",
       "0               17   [[5, 17]]  \n",
       "1               27   [[7, 27]]  \n",
       "2               15   [[4, 15]]  \n",
       "3               58   [[2, 58]]  \n",
       "4               35   [[8, 35]]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slider1_start</th>\n",
       "      <th>slider1_end</th>\n",
       "      <th>slider2_start</th>\n",
       "      <th>slider2_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   slider1_start  slider1_end  slider2_start  slider2_end\n",
       "0             39           46             27           46\n",
       "1             25           41              6           17\n",
       "2             35           37             19           26\n",
       "3             35           36             57           80\n",
       "4             10           16             15           16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data.filter_sent.values\n",
    "idxs = X_data.pred_tokens.values\n",
    "y = list(list(zip(y_data[['slider1_start', 'slider1_end']].values, \n",
    "             y_data[['slider2_start', 'slider2_end']].values)))\n",
    "\n",
    "data = list(zip(X, idxs, y))\n",
    "##Check sample values\n",
    "# print(X[0])\n",
    "# print(idxs[0])\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['On', 'Wednesday', 'guerrillas', 'had', 'kidnapped', 'a', 'cosmetic', 'surgeon', 'and', 'his', 'wife', 'while', 'they', 'were', 'on', 'their', 'way', 'home', '.']], [[5, 17]], (array([39, 46]), array([27, 46])))\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import from factslab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove embeddings and Word_to_ix dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1275\n"
     ]
    }
   ],
   "source": [
    "vocab_set = set()\n",
    "for [sent] in X:\n",
    "    for word in sent:\n",
    "        if word not in vocab_set:\n",
    "            vocab_set.add(word) \n",
    "print(\"Vocab size: {}\".format(len(vocab_set)))\n",
    "\n",
    "vocab = list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = load_glove_embedding(embed_path + 'glove.42B.300d', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the RNN-Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNRegression(embeddings = glove_embeddings, rnn_classes = LSTM, \n",
    "                      rnn_hidden_sizes = 300, num_rnn_layers=1, bidirectional=True, attention=False,\n",
    "                      regression_hidden_sizes = [32, 16], output_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_class=torch.optim.Adam\n",
    "optimizer = optimizer_class(model.parameters())\n",
    "#optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:31, 15.68it/s]\n",
      "2it [00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Overall MSE Loss: 5894.22314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:31, 16.04it/s]\n",
      "2it [00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Overall MSE Loss: 5886.68017578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:35, 14.28it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Overall MSE Loss: 5879.5390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.45it/s]\n",
      "2it [00:00, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Overall MSE Loss: 5872.353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:30, 16.37it/s]\n",
      "2it [00:00, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Overall MSE Loss: 5864.8837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:36, 13.78it/s]\n",
      "2it [00:00, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Overall MSE Loss: 5857.78955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:36, 13.59it/s]\n",
      "2it [00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Overall MSE Loss: 5851.7939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:31, 15.74it/s]\n",
      "2it [00:00, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Overall MSE Loss: 5846.443359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:31, 15.78it/s]\n",
      "2it [00:00, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Overall MSE Loss: 5840.56396484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.36it/s]\n",
      "2it [00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Overall MSE Loss: 5834.6640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:33, 15.14it/s]\n",
      "2it [00:00, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed. Overall MSE Loss: 5829.216796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:33, 15.00it/s]\n",
      "2it [00:00, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed. Overall MSE Loss: 5824.13623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:33, 14.90it/s]\n",
      "2it [00:00, 14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed. Overall MSE Loss: 5819.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:35, 14.01it/s]\n",
      "2it [00:00, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed. Overall MSE Loss: 5814.72509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:34, 14.46it/s]\n",
      "2it [00:00, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed. Overall MSE Loss: 5810.30859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.19it/s]\n",
      "2it [00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 completed. Overall MSE Loss: 5805.93017578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.24it/s]\n",
      "2it [00:00, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 completed. Overall MSE Loss: 5801.75390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.49it/s]\n",
      "2it [00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 completed. Overall MSE Loss: 5797.7431640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:32, 15.51it/s]\n",
      "2it [00:00, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 completed. Overall MSE Loss: 5793.92919921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:35, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 completed. Overall MSE Loss: 5790.26171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_obs = len(data)\n",
    "losses = []\n",
    "for epoch in range(20):\n",
    "    for i, (words, [idx], target) in tqdm(enumerate(data)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicts = model(words, idxs=idx)\n",
    "        actuals = [torch.from_numpy(arr).float().view(1,2) for arr in target]\n",
    "\n",
    "        loss_fn = MSELoss()\n",
    "        losses = []\n",
    "\n",
    "        for predicted, actual in list(zip(predicts, actuals)):\n",
    "\n",
    "            losses.append(loss_fn(predicted, actual))\n",
    "\n",
    "\n",
    "        overall_loss = sum(losses)\n",
    "        \n",
    "        \n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    losses.append(overall_loss)\n",
    "    print(\"Epoch {} completed. Overall MSE Loss: {}\".format((epoch+1), overall_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.7649,  0.6589]]), tensor([[ 0.4548,  0.2953]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_idx = 140\n",
    "\n",
    "words = data[data_idx][0]\n",
    "[idx] = data[data_idx][1]\n",
    "print(\"Predicted value\")\n",
    "model(words, idxs = idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([17, 48]), array([70, 80]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Actual value\")\n",
    "data[data_idx][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = torch.nn.Embedding(1275, 300)\n",
    "\n",
    "# indices = [[1], [2], [5], [45]]\n",
    "# indices = torch.tensor(indices, dtype=torch.long)\n",
    "# print(indices.type())\n",
    "# indices.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
